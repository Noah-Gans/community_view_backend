#!/usr/bin/env python3
"""
ğŸ—ï¸ COMMUNITY VIEW BACKEND MANAGER ğŸ—ï¸

This is the MASTER CONTROL script for your entire backend system.
Think of it as the "conductor" of an orchestra - it coordinates all the different pieces.

ğŸ¯ WHAT IT MANAGES:
â”œâ”€â”€ ğŸ” Search API (FastAPI on port 8000)
â”œâ”€â”€ ğŸ—ºï¸  Tegola Server (Manual startup - optional) 
â”œâ”€â”€ ğŸ“Š PostgreSQL Database (with PostGIS)
â”œâ”€â”€ â˜ï¸  Google Cloud Storage uploads
â”œâ”€â”€ ğŸ”„ Daily data updates (download â†’ process â†’ upload â†’ database â†’ search index)
â”œâ”€â”€ ğŸ¥ Health monitoring (every 15 minutes)
â””â”€â”€ ğŸ“§ Email notifications (daily summaries + alerts)

ğŸ›ï¸ CONTROL MODES:
- start    â†’ Start all services, then exit
- stop     â†’ Stop all services gracefully
- status   â†’ Check if services are running
- update   â†’ Run data pipeline once (manual trigger)
- daemon   â†’ Run forever with 2 AM scheduling + health checks
- health   â†’ Run health checks once

ğŸ”„ DAILY WORKFLOW (2 AM automatic):
1. Download county data (Fremont, Teton ID/WY, Lincoln, Sublette)
2. Process & standardize GeoJSON files
3. Upload to Google Cloud Storage  
4. Migrate to PostgreSQL database
5. Rebuild search index (93k+ properties)
6. Hot-reload search API
7. Email summary to noahgans@tetoncountygis.com
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“¦ IMPORTS & SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import argparse
import json
import logging
import os
import subprocess
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

# Try to import optional dependencies with helpful error messages
try:
    import schedule      # For 2 AM scheduling
    import requests     # For health checks (HTTP requests)
    import psutil       # For system process monitoring
except ImportError as e:
    print(f"âŒ Missing dependency: {e}")
    print("ğŸ’¡ Fix with: pip install schedule requests psutil")
    sys.exit(1)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ—ï¸ MAIN MANAGER CLASS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CommunityViewManager:
    """
    ğŸ›ï¸ The main controller class that manages everything
    
    INITIALIZATION FLOW:
    1. Load config.json settings
    2. Set up project paths
    3. Configure logging system
    """
    
    def __init__(self, config_path: str = "config.json"):
        """
        ğŸš€ Initialize the manager
        
        Args:
            config_path: Path to config.json file with all settings
        """
        self.config_path = config_path
        self.config = self._load_config()          # Load settings from config.json
        self.project_root = Path(__file__).parent  # Get parent directory path
        self._setup_logging()                      # Set up logging system
        
    def _load_config(self) -> Dict:
        """
        ğŸ“– Load configuration from config.json
        
        CONTAINS:
        - Email notification settings
        - Database connection info  
        - List of counties to process
        - Service ports and paths
        - Scheduling settings (2 AM time)
        
        Returns:
            Dict: Configuration dictionary
        """
        try:
            with open(self.config_path, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ Config file {self.config_path} not found")
            print("ğŸ’¡ Make sure config.json exists in the project root")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"âŒ Invalid JSON in config file: {e}")
            sys.exit(1)
    
    def _setup_logging(self):
        """
        ğŸ“ Configure the logging system
        
        CREATES:
        - Daily log files: logs/community_view_manager_YYYYMMDD.log
        - Console output with timestamps
        - INFO level logging (shows operations, not debug spam)
        
        LOG STRUCTURE:
        2025-08-06 14:30:15,123 - __main__ - INFO - ğŸ” Starting search...
        """
        log_dir = Path(self.config["paths"]["log_dir"])
        log_dir.mkdir(exist_ok=True)  # Create logs/ directory if it doesn't exist
        
        # Create daily log file with date stamp
        log_file = log_dir / f"community_view_manager_{datetime.now().strftime('%Y%m%d')}.log"
        
        logging.basicConfig(
            level=getattr(logging, self.config["general"]["log_level"]),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),      # Save to file
                logging.StreamHandler(sys.stdout)   # Show on console
            ]
        )
        self.logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ SERVICE MANAGEMENT (Start/Stop APIs)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def start_services(self) -> bool:
        """
        ğŸš€ Start all backend services
        
        WHAT IT DOES:
        1. Runs ./scripts/start_services.sh
        2. That script starts:
           - Search API (Python FastAPI on port 8000)
           - Tegola Server (skipped - start manually if needed)
        3. Saves process IDs to logs/search_api.pid
        4. Verifies services are responding with HTTP health checks
        
        Returns:
            bool: True if all services started successfully
        """
        self.logger.info("ğŸš€ Starting Community View Backend Services")
        
        try:
            # Run the bash script with debug output
            script_path = "./scripts/start_services.sh"
            
            self.logger.info(f"ğŸ“ Running script: {script_path}")
            self.logger.info(f"ğŸ“ Working directory: {self.project_root}")
            
            # Use Popen for better control over the subprocess
            import subprocess
            process = subprocess.Popen(
                ["bash", "-x", script_path],
                cwd=self.project_root,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,  # Merge stderr into stdout
                text=True,
                bufsize=1,  # Line buffered
                universal_newlines=True
            )
            
            # Stream output in real-time with timeout
            import time
            start_time = time.time()
            timeout = 120
            output_lines = []
            
            while True:
                # Check if process has finished
                if process.poll() is not None:
                    # Process finished, read any remaining output
                    remaining_output = process.stdout.read()
                    if remaining_output:
                        output_lines.append(remaining_output)
                        self.logger.info(f"ğŸ“„ Script output: {remaining_output.strip()}")
                    break
                
                # Check for timeout
                if time.time() - start_time > timeout:
                    self.logger.error(f"âŒ Script timeout after {timeout} seconds, terminating...")
                    process.terminate()
                    time.sleep(2)
                    if process.poll() is None:
                        process.kill()
                    return False
                
                # Read available output (non-blocking)
                try:
                    output = process.stdout.readline()
                    if output:
                        output_lines.append(output)
                        self.logger.info(f"ğŸ“„ Script output: {output.strip()}")
                except:
                    pass
                
                time.sleep(0.1)  # Small delay to prevent busy waiting
            
            exit_code = process.returncode
            all_output = ''.join(output_lines)
            
            self.logger.info(f"ğŸ”§ Script exit code: {exit_code}")
            
            if exit_code == 0:
                self.logger.info("âœ… Services started successfully")
                return True
            else:
                self.logger.error(f"âŒ Failed to start services (exit code: {exit_code})")
                # Check for specific port conflict errors
                if "address already in use" in all_output:
                    self.logger.warning("âš ï¸  Port conflicts detected - services may already be running")
                    # Check if services are actually running despite the error
                    status = self.check_service_status()
                    if status.get('search_api_running') and status.get('tegola_running'):
                        self.logger.info("âœ… Services are actually running despite startup errors")
                        return True
                return False
                
        except subprocess.TimeoutExpired:
            self.logger.error("âŒ Service startup timed out after 120 seconds")
            return False
        except Exception as e:
            self.logger.error(f"âŒ Error starting services: {e}")
            return False
    
    def stop_services(self) -> bool:
        """
        ğŸ›‘ Stop all backend services gracefully
        
        WHAT IT DOES:
        1. Runs ./scripts/stop_services.sh
        2. That script:
           - Reads PID files (logs/search_api.pid, logs/tegola.pid)
           - Sends SIGTERM to processes (graceful shutdown)
           - If they don't stop, sends SIGKILL (force kill)
           - Cleans up PID files
        
        Returns:
            bool: True if services stopped successfully
        """
        self.logger.info("ğŸ›‘ Stopping Community View Backend Services")
        
        try:
            result = subprocess.run(
                ["./scripts/stop_services.sh"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=30  # Stopping should be faster than starting
            )
            
            if result.returncode == 0:
                self.logger.info("âœ… Services stopped successfully")
                return True
            else:
                self.logger.error(f"âŒ Failed to stop services: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Error stopping services: {e}")
            return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¥ HEALTH MONITORING (Check if services are alive)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def check_service_status(self) -> Dict[str, bool]:
        """
        ğŸ¥ Check if all services are running and healthy
        
        CHECKS:
        - Search API: HTTP GET to localhost:8000/health
                   - Tegola: HTTP GET to localhost:8081/maps  
        - Database: PostgreSQL connection test (if enabled in config)
        
        Returns:
            Dict[str, bool]: {"search_api": True, "tegola": False, "database": True}
        """
        status = {}
        
        # ğŸ” CHECK SEARCH API
        try:
            response = requests.get(
                self.config["health_checks"]["search_api_endpoint"],
                timeout=5  # Give up after 5 seconds
            )
            status["search_api"] = response.status_code == 200
        except:
            status["search_api"] = False
        
        # ğŸ—ºï¸ CHECK TEGOLA SERVER (optional - only if manually started)
        try:
            response = requests.get(
                self.config["health_checks"]["tegola_endpoint"],
                timeout=5
            )
            status["tegola"] = response.status_code == 200
        except:
            # Tegola is optional - not considered an error if not running
            status["tegola"] = False
        
        # ğŸ—„ï¸ CHECK DATABASE (if enabled)
        if self.config["health_checks"]["database_check"]:
            status["database"] = self._check_database()
        
        return status
    
    def _check_database(self) -> bool:
        """
        ğŸ—„ï¸ Test PostgreSQL database connectivity
        
        WHAT IT DOES:
        1. Attempts to connect to PostgreSQL using config.json settings
        2. Immediately closes connection (just a ping test)
        3. Returns True if connection succeeded
        
        Returns:
            bool: True if database is reachable
        """
        try:
            import psycopg2
            db_config = self.config["database"]
            self.logger.info(f"Attempting database connection to {db_config['host']}:{db_config['port']} as {db_config['user']}")
            conn = psycopg2.connect(
                host=db_config["host"],
                port=db_config["port"],
                database=db_config["database"],
                user=db_config["user"],
                password=db_config.get("password", ""),  # Add password support
                connect_timeout=5  # Quick timeout
            )
            conn.close()
            self.logger.info("Database connection successful!")
            return True
        except Exception as e:
            self.logger.error(f"Database connection failed: {str(e)}")
            return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ï¿½ï¿½ DATA PROCESSING PIPELINE (The Big Daily Update)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def run_data_update_cycle(self) -> Dict[str, any]:
        """
        ğŸ”„ Run the complete data update cycle
        
        THIS IS THE BIG ONE! This is what runs at 2 AM every day.
        
        FULL PIPELINE:
        1. ğŸ“¥ Download county data for all 5 counties
        2. ğŸ”§ Process & standardize GeoJSON files  
        3. â˜ï¸  Upload to Google Cloud Storage
        4. ğŸ—„ï¸ Migrate to PostgreSQL database with PostGIS
        5. ğŸ” Rebuild search index (93k+ properties)
        6. ğŸ”„ Hot-reload search API
        7. ğŸ“§ Send email notification with results
        
        COUNTIES PROCESSED:
        - fremont_county_wy
        - teton_county_id  
        - lincoln_county_wy
        - sublette_county_wy
        - teton_county_wy
        
        Returns:
            Dict: Results summary with timing, success/failure counts, errors
        """
        self.logger.info("ğŸ”„ Starting data update cycle")
        start_time = datetime.now()
        
        # Initialize results tracking
        results = {
            "start_time": start_time.isoformat(),
            "counties_processed": [],
            "counties_failed": [],
            "search_index_updated": False,
            "gcs_uploads": [],
            "database_migrations": [],
            "errors": []
        }
        
        try:
            # ğŸ ACTIVATE VIRTUAL ENVIRONMENT
            venv_activate = self.project_root / self.config["paths"]["venv_path"] / "bin" / "activate"
            
            # ğŸ PROCESS ALL COUNTIES WITH POSTGIS MIGRATION
            self.logger.info("ğŸ Starting ownership pipeline for all counties")
            
            try:
                # ğŸš€ BUILD COMMAND
                # This runs: python tile_processing/ownership_pipeline.py --all --migrate-to-postgis
                cmd = [
                    "bash", "-c", 
                    f"source {venv_activate} && python tile_processing/ownership_pipeline.py --all --migrate-to-postgis"
                ]
                
                self.logger.info("ğŸ”„ Running: tile_processing/ownership_pipeline.py --all --migrate-to-postgis")
                
                # ğŸ“º RUN WITH REAL-TIME OUTPUT STREAMING
                # This lets you see what tile_processing is doing as it happens
                process = subprocess.Popen(
                    cmd,
                    cwd=self.project_root,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,  # Combine stderr into stdout
                    text=True,
                    bufsize=1,                 # Line buffering
                    universal_newlines=True
                )
                
                # ğŸ“– STREAM OUTPUT IN REAL-TIME
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break  # Process finished
                    if output:
                        # Show output both in logs and console
                        output_clean = output.strip()
                        self.logger.info(f"ğŸ“¦ {output_clean}")
                        print(f"ğŸ“¦ {output_clean}")  # Real-time console output
                
                # âœ… CHECK RESULTS
                return_code = process.poll()
                
                if return_code == 0:
                    # Success! Mark all counties as processed
                    results["counties_processed"] = self.config["counties"]
                    results["database_migrations"] = self.config["counties"]
                    self.logger.info("âœ… Successfully processed all counties with PostGIS migration")
                else:
                    # Failed! Mark all counties as failed
                    results["counties_failed"] = self.config["counties"]
                    error_msg = "Failed to process counties with ownership pipeline"
                    results["errors"].append(error_msg)
                    self.logger.error(error_msg)
                    
            except Exception as e:
                error_msg = f"Error running ownership pipeline: {str(e)}"
                results["counties_failed"] = self.config["counties"]
                results["errors"].append(error_msg)
                self.logger.error(error_msg)
            
            # ğŸ” REBUILD SEARCH INDEX
            self.logger.info("ğŸ” Rebuilding search index")
            try:
                cmd = [
                    "bash", "-c",
                    f"source {venv_activate} && python search_api/search_file_generator.py"
                ]
                
                result = subprocess.run(
                    cmd,
                    cwd=self.project_root,
                    capture_output=True,
                    text=True,
                    timeout=600  # 10 minutes should be enough
                )
                
                if result.returncode == 0:
                    results["search_index_updated"] = True
                    self.logger.info("âœ… Search index rebuilt successfully")
                    
                    # ğŸ”„ HOT-RELOAD SEARCH API
                    try:
                        response = requests.post("http://localhost:8000/internal/reload-search-index")
                        if response.status_code == 200:
                            self.logger.info("âœ… Search API reloaded successfully")
                        else:
                            self.logger.warning("âš ï¸ Search API reload returned non-200 status")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Could not reload search API: {e}")
                else:
                    error_msg = f"Failed to rebuild search index: {result.stderr}"
                    results["errors"].append(error_msg)
                    self.logger.error(error_msg)
                    
            except Exception as e:
                error_msg = f"Error rebuilding search index: {str(e)}"
                results["errors"].append(error_msg)
                self.logger.error(error_msg)
        
        except Exception as e:
            error_msg = f"Critical error in data update cycle: {str(e)}"
            results["errors"].append(error_msg)
            self.logger.error(error_msg)
        
        # ğŸ“Š CALCULATE FINAL TIMING
        end_time = datetime.now()
        results["end_time"] = end_time.isoformat()
        results["duration_minutes"] = (end_time - start_time).total_seconds() / 60
        
        self.logger.info(f"ğŸ‰ Data update cycle completed in {results['duration_minutes']:.1f} minutes")
        return results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“§ EMAIL NOTIFICATIONS (Keep you informed)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def send_notification_email(self, subject: str, content: str, is_error: bool = False):
        """
        ğŸ“§ Send email notification
        
        WHEN EMAILS ARE SENT:
        - Daily data update summaries (always)
        - Health check alerts (only when problems detected)
        
        EMAIL CONTENT INCLUDES:
        - Counties processed successfully
        - Counties that failed  
        - Processing duration
        - Error details
        - Service status
        
        Args:
            subject: Email subject line
            content: Email body text  
            is_error: True for alerts, False for normal summaries
        """
        try:
            email = self.config["general"]["notification_email"]
            if not email:
                self.logger.warning("No notification email configured")
                return
            
            # Import email modules only when needed (avoids startup issues)
            import smtplib
            from email.mime.text import MimeText
            from email.mime.multipart import MimeMultipart
            
            # ğŸ“ CREATE EMAIL CONTENT
            msg = MimeMultipart()
            msg['From'] = "Community View Backend <noreply@tetoncountygis.com>"
            msg['To'] = email
            msg['Subject'] = f"{'ğŸš¨ ' if is_error else 'ğŸ“Š '}Community View Backend - {subject}"
            
            msg.attach(MimeText(content, 'plain'))
            
            # ğŸ“¤ SEND EMAIL
            # Gmail SMTP settings for tetoncountygis.com
            smtp_server = "smtp.gmail.com"
            smtp_port = 587
            sender_email = "noahgans@tetoncountygis.com"  # Your Gmail address
            sender_password = "your-app-password"  # You'll need to set this up
            
            # Create SMTP connection
            server = smtplib.SMTP(smtp_server, smtp_port)
            server.starttls()
            server.login(sender_email, sender_password)
            
            # Send email
            server.send_message(msg)
            server.quit()
            
            self.logger.info(f"ğŸ“§ Email sent successfully: {subject}")
            
        except Exception as e:
            self.logger.error(f"Failed to send email notification: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– DAEMON MODE (24/7 Operation with Scheduling)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def run_daemon(self):
        """
        ğŸ¤– Run as daemon with scheduled tasks
        
        THIS IS PRODUCTION MODE! Runs forever and handles:
        
        ğŸ“… SCHEDULED TASKS:
        - Daily data updates at 2:00 AM
        - Health checks every 15 minutes
        
        ğŸ”„ LIFECYCLE:
        1. Start all services (Search API + Tegola)
        2. Schedule recurring tasks
        3. Run initial health check
        4. Enter infinite loop checking for scheduled tasks
        5. Run until manually stopped (Ctrl+C)
        
        PERFECT FOR:
        - Production VM deployment
        - 24/7 automated operation
        - Systemd service integration
        """
        self.logger.info("ğŸ¤– Starting Community View Backend Daemon")
        
        # Start services first
        if not self.start_services():
            self.logger.error("âŒ Failed to start services")
            return
            
        # Schedule tasks
        update_time = self.config["scheduling"]["data_update_time"]
        schedule.every().day.at(update_time).do(self._scheduled_data_update)
        schedule.every(15).minutes.do(self._scheduled_health_check)
        
        self.logger.info(f"ğŸ“… Scheduled daily data update at {update_time}")
        self.logger.info("ğŸ“… Scheduled health checks every 15 minutes")
        
        # Run initial health check
        self._scheduled_health_check()
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)
        except KeyboardInterrupt:
            self.logger.info("ğŸ›‘ Daemon stopped by user")
        except Exception as e:
            self.logger.error(f"âŒ Daemon error: {e}")
    
    def _scheduled_data_update(self):
        """ğŸ”„ Scheduled data update - called daily at 2 AM"""
        self.logger.info("ğŸ”„ Running scheduled data update...")
        try:
            result = self.run_data_update_cycle()
            self.logger.info(f"âœ… Scheduled update completed: {result}")
        except Exception as e:
            self.logger.error(f"âŒ Scheduled update failed: {e}")
            self.send_notification_email(
                "Community View - Scheduled Update Failed",
                f"Error during scheduled data update: {e}",
                is_error=True
            )
    
    def _scheduled_health_check(self):
        """ğŸ¥ Scheduled health check - called every 15 minutes"""
        self.logger.info("ğŸ¥ Running scheduled health check...")
        try:
            # Use the existing health check method
            status = self.check_service_status()
            self.logger.info(f"âœ… Health check completed: {status}")
        except Exception as e:
            self.logger.error(f"âŒ Health check failed: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ COMMAND LINE INTERFACE (How you control everything)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """
    ğŸ¯ Main entry point - handles command line arguments
    
    AVAILABLE COMMANDS:
    
    ğŸš€ python community_view_manager.py start
       - Start Search API + Tegola
       - Exit immediately (services keep running)
       - Use for: Manual startup, development
    
    ğŸ›‘ python community_view_manager.py stop  
       - Stop all services gracefully
       - Clean up PID files
       - Use for: Manual shutdown
    
    ğŸ“Š python community_view_manager.py status
       - Check if services are running
       - Returns exit code 0 if healthy, 1 if problems
       - Use for: Monitoring scripts, health checks
    
    ğŸ”„ python community_view_manager.py update
       - Run full data pipeline once
       - Process all counties, rebuild search index
       - Send email notification
       - Use for: Manual data updates, testing
    
    ğŸ¤– python community_view_manager.py daemon
       - Start services + run forever with scheduling
       - Daily 2 AM updates + 15-minute health checks
       - Email notifications for everything
       - Use for: Production deployment
    
    ğŸ¥ python community_view_manager.py health
       - Run comprehensive health checks once
       - Check Search API, Tegola, Database
       - Use for: Troubleshooting, monitoring
    """
    parser = argparse.ArgumentParser(description="Community View Backend Manager")
    parser.add_argument("command", choices=["start", "stop", "status", "update", "daemon", "health"],
                      help="Command to execute")
    parser.add_argument("--config", default="config.json", help="Config file path")
    
    args = parser.parse_args()
    
    # ğŸ—ï¸ CREATE MANAGER INSTANCE
    manager = CommunityViewManager(args.config)
    
    # ğŸ›ï¸ EXECUTE COMMAND
    if args.command == "start":
        success = manager.start_services()
        sys.exit(0 if success else 1)
        
    elif args.command == "stop":
        success = manager.stop_services()
        sys.exit(0 if success else 1)
        
    elif args.command == "status":
        status = manager.check_service_status()
        print("ğŸ¥ Service Status:")
        for service, is_healthy in status.items():
            status_icon = "âœ…" if is_healthy else "âŒ"
            print(f"  {status_icon} {service}: {'Running' if is_healthy else 'Not Running'}")
        
        all_healthy = all(status.values())
        print(f"\n{'âœ… All services healthy' if all_healthy else 'âš ï¸ Some services have issues'}")
        sys.exit(0 if all_healthy else 1)
        
    elif args.command == "update":
        results = manager.run_data_update_cycle()
        print(f"âœ… Data update completed in {results['duration_minutes']:.1f} minutes")
        if results['errors']:
            print(f"âš ï¸ {len(results['errors'])} errors occurred")
            for error in results['errors']:
                print(f"  - {error}")
        
    elif args.command == "health":
        health_report = manager.run_health_checks()
        if health_report["all_healthy"]:
            print("âœ… All systems healthy")
        else:
            print("âš ï¸ Health issues detected:")
            for issue in health_report["issues"]:
                print(f"  - {issue}")
        sys.exit(0 if health_report["all_healthy"] else 1)
        
    elif args.command == "daemon":
        manager.run_daemon()

if __name__ == "__main__":
    main()